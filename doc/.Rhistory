ground_truth_category,
predicted_category,
text) %>%
mutate(count = sapply(hm_data$text, wordcount)) %>%
filter(gender %in% c("m", "f")) %>%
filter(marital %in% c("single", "married")) %>%
filter(parenthood %in% c("n", "y")) %>%
filter(reflection_period %in% c("24h", "3m")) %>%
#fliter(predicted_category %in% c("affection","exercise","bonding","leisure","achievement","enjoy_the_moment","achievement")) %>%
mutate(reflection_period = fct_recode(reflection_period,
months_3 = "3m", hours_24 = "24h"))
hm.list=hm_data[2:(nrow(hm_data)-1), ]
sent.pre=hm_data$cleaned_hm[1:(nrow(hm_data)-2)]
sent.post=hm_data$cleaned_hm[3:(nrow(hm_data)-1)]
hm.list$snipets=paste(sent.pre, hm.list$clean_hm, sent.post, sep=" ")
docs_c <- Corpus(VectorSource(hm.list$snipets))
hlw <- DocumentTermMatrix(docs_c)
burnin_c <- 400
iter_c <- 200
thin_c <- 50
seed_c <-list(2003,5,63,100001,765)
nstart_c <- 5
best_c <- TRUE
# number of topics
k_c <- 7
# run LDA using Gibbs sampling
ldaOut_c <-LDA(hlw, k, method="Gibbs", control=list(nstart=nstart_c,
seed = seed_c, best=best_c,
burnin = burnin_c, iter = iter_c,
thin=thin_c))
burnin_c <- 400
iter_c <- 200
thin_c <- 50
seed_c <-list(2003,5,63,100001,765)
nstart_c <- 5
best_c <- TRUE
# number of topics
k_c <- 7
# run LDA using Gibbs sampling
ldaOut_c <-LDA(hlw, k_c, method="Gibbs", control=list(nstart=nstart_c,
seed = seed_c, best=best_c,
burnin = burnin_c, iter = iter_c,
thin=thin_c))
ldaOut.topics.c <- as.matrix(topics(ldaOut_c))
ldaOut.topics.c <- as.matrix(topics(ldaOut_c))
ldaOut.terms_c <- as.matrix(terms(ldaOut_c,20))
ldaOut.topics.c
ldaOut.terms_c
topicProbabilities_c <- as.data.frame(ldaOut_c@gamma)
terms.beta_c=ldaOut_c@beta
terms.beta_c=scale(terms.beta_c)
topics.terms_c=NULL
for(i in 1:k_c){
topics.terms_c=rbind(topics.terms_c, ldaOut_c@terms[order(terms.beta_c[i,], decreasing = TRUE)[1:15]])
}
topics.terms_c
View(topics.terms_c)
View(topics.terms_c)
topics.terms_c=NULL
for(i in 1:k_c){
topics.terms_c=rbind(topics.terms_c, ldaOut_c@terms[order(terms.beta_c[i,], decreasing = TRUE)[1:20]])
}
corpus.list=hm_data[2:(nrow(hm_data)-1), ]
words.pre=hm_data$cleaned_hm[1:(nrow(hm_data)-2)]
words.post=hm_data$cleaned_hm[3:(nrow(hm_data)-1)]
corpus.list$snipets=paste(words.pre, corpus.list$cleaned_hm, words.post, sep=" ")
dtm <- DocumentTermMatrix(Corpus(VectorSource(corpus.list$snipets)))
rowTotals <- apply(dtm , 1, sum)
burnin <- 400
iter <- 20
thin <- 500
seed <-list(2003,5,63,100001,765)
nstart <- 5
best <- TRUE
# number of topics
k <- 7
# run LDA using Gibbs sampling
ldaOut <-LDA(dtm, k, method="Gibbs", control=list(nstart=nstart,
seed = seed, best=best,
burnin = burnin, iter = iter,
thin=thin))
burnin <- 400
iter <- 200
thin <- 50
seed <-list(2003,5,63,100001,765)
nstart <- 5
best <- TRUE
# number of topics
k <- 7
# run LDA using Gibbs sampling
ldaOut <-LDA(dtm, k, method="Gibbs", control=list(nstart=nstart,
seed = seed, best=best,
burnin = burnin, iter = iter,
thin=thin))
ldaOut.topics <- as.matrix(topics(ldaOut))
ldaOut.terms <- as.matrix(terms(ldaOut,20))
topicProbabilities <- as.data.frame(ldaOut@gamma)
terms.beta=ldaOut@beta
terms.beta=scale(terms.beta)
topics.terms=NULL
for(i in 1:k){
topics.terms=rbind(topics.terms, ldaOut@terms[order(terms.beta[i,], decreasing = TRUE)[1:7]])
}
View(topics.terms_c)
View(topics.terms_c)
View(topics.terms)
View(topics.terms)
topics.terms=NULL
for(i in 1:k){
topics.terms=rbind(topics.terms, ldaOut@terms[order(terms.beta[i,], decreasing = TRUE)[1:15]])
}
View(topics.terms)
View(topics.terms)
View(ldaOut.topics)
View(ldaOut.topics)
topics.hash=c("Economy", "Patriotism", "Trust", "Liberty", "Government", "CountryRelationship", "Temporal", "Election", "Work&Life", "Future")
corpus.list$ldatopic=as.vector(ldaOut.topics)
corpus.list$ldahash=topics.hash[ldaOut.topics]
colnames(topicProbabilities)=topics.hash
topics.hash=c("Economy", "Patriotism", "Trust", "Liberty", "Government", "CountryRelationship", "Temporal")
corpus.list$ldatopic=as.vector(ldaOut.topics)
corpus.list$ldahash=topics.hash[ldaOut.topics]
colnames(topicProbabilities)=topics.hash
corpus.list.df=cbind(corpus.list, topicProbabilities)
View(corpus.list.df)
View(corpus.list.df)
View(corpus.list.df)
View(corpus.list.df)
par(mar=c(1,1,1,1))
topic.summary=tbl_df(corpus.list.df)%>%
select(Index, Economy:Future)%>%
group_by(Index)%>%
summarise_each(funs(mean))
par(mar=c(1,1,1,1))
topic.summary=tbl_df(corpus.list.df)%>%
select(Index, Economy:temporal)%>%
group_by(Index)%>%
summarise_each(funs(mean))
par(mar=c(1,1,1,1))
topic.summary=tbl_df(corpus.list.df)%>%
select(Index, Economy:Temporal)%>%
group_by(Index)%>%
summarise_each(funs(mean))
par(mar=c(1,1,1,1))
topic.summary=tbl_df(corpus.list.df)%>%
select(reflaction_period, Economy:Temporal)%>%
group_by(reflaction_period)%>%
summarise_each(funs(mean))
topic.summary=tbl_df(corpus.list.df)%>%
select(reflection_period, Economy:Temporal)%>%
group_by(reflection_period)%>%
summarise_each(funs(mean))
par(mar=c(1,1,1,1))
topic.summary=tbl_df(corpus.list.df)%>%
select(reflection_period, Economy:Temporal)%>%
group_by(reflection_period)%>%
summarise_each(funs(mean))
topic.summary=as.data.frame(topic.summary)
heatmap.2(as.matrix(topic.summary[,-1]), Rowv = FALSE,
scale = "column", key=F,
col = bluered(100),
cexRow = 0.9, cexCol = 0.9, margins = c(8, 8),
trace = "none", density.info = "none")
?heatmap.2
topics.hash=c("Economy", "Patriotism", "Trust", "Liberty", "Government", "CountryRelationship", "Temporal")
corpus.list$ldatopic=as.vector(ldaOut.topics)
corpus.list$ldahash=topics.hash[ldaOut.topics]
colnames(topicProbabilities)=topics.hash
corpus.list.df=cbind(corpus.list, topicProbabilities)
par(mar=c(1,1,1,1))
topic.summary=tbl_df(corpus.list.df)%>%
select(predicted_category, Economy:Temporal)%>%
group_by(predicted_category)%>%
summarise_each(funs(mean))
topic.summary=as.data.frame(topic.summary)
heatmap.2(as.matrix(topic.summary[,-1]), Rowv = FALSE,
scale = "column", key=F,
col = bluered(100),
cexRow = 0.9, cexCol = 0.9, margins = c(8, 8),
trace = "none", density.info = "none")
topic.summary
row_names(topic.summary)<-topic.summary$predicted_category
rownames(topic.summary)<-topic.summary$predicted_category
topics.hash=c("Economy", "Patriotism", "Trust", "Liberty", "Government", "CountryRelationship", "Temporal")
corpus.list$ldatopic=as.vector(ldaOut.topics)
corpus.list$ldahash=topics.hash[ldaOut.topics]
colnames(topicProbabilities)=topics.hash
corpus.list.df=cbind(corpus.list, topicProbabilities)
par(mar=c(1,1,1,1))
topic.summary=tbl_df(corpus.list.df)%>%
select(predicted_category, Economy:Temporal)%>%
group_by(predicted_category)%>%
summarise_each(funs(mean))
topic.summary=as.data.frame(topic.summary)
rownames(topic.summary)<-topic.summary$predicted_category
heatmap.2(as.matrix(topic.summary[,-1]), Rowv = FALSE,
scale = "column", key=F,
col = bluered(100),
cexRow = 0.9, cexCol = 0.9, margins = c(8, 8),
trace = "none", density.info = "none")
topics.hash=c("Leisure", "achievement", "nature", "affection_2", "enjoy_the_moment", "affection_2", "bonding")
corpus.list$ldatopic=as.vector(ldaOut.topics)
corpus.list$ldahash=topics.hash[ldaOut.topics]
colnames(topicProbabilities)=topics.hash
corpus.list.df=cbind(corpus.list, topicProbabilities)
par(mar=c(1,1,1,1))
topic.summary=tbl_df(corpus.list.df)%>%
select(predicted_category, Economy:Temporal)%>%
group_by(predicted_category)%>%
summarise_each(funs(mean))
topics.hash=c("Leisure", "achievement", "nature", "affection_2", "enjoy_the_moment", "affection_1", "bonding")
corpus.list$ldatopic=as.vector(ldaOut.topics)
corpus.list$ldahash=topics.hash[ldaOut.topics]
colnames(topicProbabilities)=topics.hash
corpus.list.df=cbind(corpus.list, topicProbabilities)
par(mar=c(1,1,1,1))
topic.summary=tbl_df(corpus.list.df)%>%
select(predicted_category, Economy:Temporal)%>%
group_by(predicted_category)%>%
summarise_each(funs(mean))
topics.hash=c("Leisure", "achievement", "nature", "affection_2", "enjoy_the_moment", "affection_1", "bonding")
corpus.list$ldatopic=as.vector(ldaOut.topics)
corpus.list$ldahash=topics.hash[ldaOut.topics]
colnames(topicProbabilities)=topics.hash
corpus.list.df=cbind(corpus.list, topicProbabilities)
par(mar=c(1,1,1,1))
topic.summary=tbl_df(corpus.list.df)%>%
select(predicted_category, Leisure:bonding)%>%
group_by(predicted_category)%>%
summarise_each(funs(mean))
topic.summary=as.data.frame(topic.summary)
rownames(topic.summary)<-topic.summary$predicted_category
heatmap.2(as.matrix(topic.summary[,-1]), Rowv = FALSE,
scale = "column", key=F,
col = bluered(100),
cexRow = 0.9, cexCol = 0.9, margins = c(8, 8),
trace = "none", density.info = "none")
topicProbabilities <- as.data.frame(ldaOut@gamma)
terms.beta=ldaOut@beta
terms.beta=scale(terms.beta)
topics.terms=NULL
for(i in 1:k){
topics.terms=rbind(topics.terms, ldaOut@terms[order(terms.beta[i,], decreasing = TRUE)[1:20]])
}
topics.hash=c("Leisure", "achievement", "nature", "affection_2", "enjoy_the_moment", "affection_1", "bonding")
corpus.list$ldatopic=as.vector(ldaOut.topics)
corpus.list$ldahash=topics.hash[ldaOut.topics]
colnames(topicProbabilities)=topics.hash
corpus.list.df=cbind(corpus.list, topicProbabilities)
par(mar=c(1,1,1,1))
topic.summary=tbl_df(corpus.list.df)%>%
select(predicted_category, Leisure:bonding)%>%
group_by(predicted_category)%>%
summarise_each(funs(mean))
topic.summary=as.data.frame(topic.summary)
rownames(topic.summary)<-topic.summary$predicted_category
heatmap.2(as.matrix(topic.summary[,-1]), Rowv = FALSE,
scale = "column", key=F,
col = bluered(100),
cexRow = 0.9, cexCol = 0.9, margins = c(8, 8),
trace = "none", density.info = "none")
topic.summary
par(mar=c(1,1,1,1))
topic.summary=tbl_df(corpus.list.df)%>%
select(replection_period, Leisure:bonding)%>%
group_by(replection_period)%>%
summarise_each(funs(mean))
par(mar=c(1,1,1,1))
topic.summary=tbl_df(corpus.list.df)%>%
select(reflection_period, Leisure:bonding)%>%
group_by(reflection_period)%>%
summarise_each(funs(mean))
topic.summary=as.data.frame(topic.summary)
rownames(topic.summary)<-topic.summary$reflection_period
heatmap.2(as.matrix(topic.summary[,-1]), Rowv = FALSE,
scale = "column", key=F,
col = bluered(100),
cexRow = 0.9, cexCol = 0.9, margins = c(8, 8),
trace = "none", density.info = "none")
par(mar=c(1,1,1,1))
topic.summary=tbl_df(corpus.list.df)%>%
select(gender, Leisure:bonding)%>%
group_by(gender)%>%
summarise_each(funs(mean))
topic.summary=as.data.frame(topic.summary)
rownames(topic.summary)<-topic.summary$gender
heatmap.2(as.matrix(topic.summary[,-1]), Rowv = FALSE,
scale = "column", key=F,
col = bluered(100),
cexRow = 0.9, cexCol = 0.9, margins = c(8, 8),
trace = "none", density.info = "none")
hm.list=hm_data[2:(nrow(hm_data)-1), ]
sent.pre=hm_data$text[1:(nrow(hm_data)-2)]
sent.post=hm_data$text[3:(nrow(hm_data)-1)]
hm.list$snipets=paste(sent.pre, hm.list$text, sent.post, sep=" ")
docs_c <- Corpus(VectorSource(hm.list$snipets))
hlw <- DocumentTermMatrix(docs_c)
## run LDA for all inaugual speeches
burnin_c <- 400
iter_c <- 200
thin_c <- 50
seed_c <-list(2003,5,63,100001,765)
nstart_c <- 5
best_c <- TRUE
# number of topics
k_c <- 7
# run LDA using Gibbs sampling
ldaOut_c <-LDA(hlw, k_c, method="Gibbs", control=list(nstart=nstart_c,
seed = seed_c, best=best_c,
burnin = burnin_c, iter = iter_c,
thin=thin_c))
ldaOut.topics.c <- as.matrix(topics(ldaOut_c))
# top 20 terms in each topic
ldaOut.terms_c <- as.matrix(terms(ldaOut_c,20))
# probabilities associated with each topic assignment
topicProbabilities_c <- as.data.frame(ldaOut_c@gamma)
terms.beta_c=ldaOut_c@beta
terms.beta_c=scale(terms.beta_c)
topics.terms_c=NULL
for(i in 1:k_c){
topics.terms_c=rbind(topics.terms_c, ldaOut_c@terms[order(terms.beta_c[i,], decreasing = TRUE)[1:20]])
}
ldaOut.topics.c <- as.matrix(topics(ldaOut_c))
# top 20 terms in each topic
ldaOut.terms_c <- as.matrix(terms(ldaOut_c,20))
# probabilities associated with each topic assignment
topicProbabilities_c <- as.data.frame(ldaOut_c@gamma)
terms.beta_c=ldaOut_c@beta
terms.beta_c=scale(terms.beta_c)
topics.terms_c=NULL
for(i in 1:k_c){
topics.terms_c=rbind(topics.terms_c, ldaOut_c@terms[order(terms.beta_c[i,], decreasing = TRUE)[1:20]])
}
ldaOut.topics_c <- as.matrix(topics(ldaOut_c))
# top 20 terms in each topic
ldaOut.terms_c <- as.matrix(terms(ldaOut_c,20))
# probabilities associated with each topic assignment
topicProbabilities_c <- as.data.frame(ldaOut_c@gamma)
terms.beta_c=ldaOut_c@beta
terms.beta_c=scale(terms.beta_c)
topics.terms_c=NULL
for(i in 1:k_c){
topics.terms_c=rbind(topics.terms_c, ldaOut_c@terms[order(terms.beta_c[i,], decreasing = TRUE)[1:20]])
}
topics.hash=c("Leisure", "achievement", "nature", "affection_2", "enjoy_the_moment", "affection_1", "bonding")
hm.list$ldatopic=as.vector(ldaOut.topics_c)
hm.list$ldahash=topics.hash[ldaOut.topics_c]
colnames(topicProbabilities_c)=topics.hash
hm.list.df=cbind(hm.list, topicProbabilities_c)
par(mar=c(1,1,1,1))
topic.summary_c=tbl_df(hm.list.df)%>%
select(predicted_category, Leisure:bonding)%>%
group_by(predicted_category)%>%
summarise_each(funs(mean))
topic.summary_c=as.data.frame(topic.summary_c)
rownames(topic.summary_c)<-topic.summary_c$predicted_category
heatmap.2(as.matrix(topic.summary_c[,-1]), Rowv = FALSE,
scale = "column", key=F,
col = bluered(100),
cexRow = 0.9, cexCol = 0.9, margins = c(8, 8),
trace = "none", density.info = "none")
#LDA
```{r}
topics.hash=c("Leisure", "achievement", "nature", "affection_2", "enjoy_the_moment", "affection_1", "bonding")
hm.list$ldatopic=as.vector(ldaOut.topics_c)
hm.list$ldahash=topics.hash[ldaOut.topics_c]
colnames(topicProbabilities_c)=topics.hash
hm.list.df=cbind(hm.list, topicProbabilities_c)
par(mar=c(1,1,1,1))
topic.summary_c=tbl_df(hm.list.df)%>%
select(predicted_category, Leisure:bonding)%>%
group_by(predicted_category)%>%
summarise_each(funs(mean))
topic.summary_c=as.data.frame(topic.summary_c)
rownames(topic.summary_c)<-topic.summary_c$predicted_category
heatmap.2(as.matrix(topic.summary_c[,-1]), Rowv = FALSE,
scale = "column", key=F,
col = bluered(100),
cexRow = 0.9, cexCol = 0.9, margins = c(8, 8),
trace = "none", density.info = "none")
par(mar=c(1,1,1,1))
topic.summary=tbl_df(corpus.list.df)%>%
select(reflection_period, Leisure:bonding)%>%
group_by(reflection_period)%>%
summarise_each(funs(mean))
topic.summary=as.data.frame(topic.summary)
rownames(topic.summary)<-topic.summary$reflection_period
heatmap.2(as.matrix(topic.summary[,-1]), Rowv = FALSE,
scale = "column", key=F,
col = bluered(100),
cexRow = 0.9, cexCol = 0.9, margins = c(8, 8),
trace = "none", density.info = "none")
par(mar=c(1,1,1,1))
topic.summary=tbl_df(corpus.list.df)%>%
select(gender, Leisure:bonding)%>%
group_by(gender)%>%
summarise_each(funs(mean))
topic.summary=as.data.frame(topic.summary)
rownames(topic.summary)<-topic.summary$gender
heatmap.2(as.matrix(topic.summary[,-1]), Rowv = FALSE,
scale = "column", key=F,
col = bluered(100),
cexRow = 0.9, cexCol = 0.9, margins = c(8, 8),
trace = "none", density.info = "none")
par(mar=c(1,1,1,1))
topic.summary=tbl_df(corpus.list.df)%>%
select(predicted_category, Leisure:bonding)%>%
group_by(predicted_category)%>%
summarise_each(funs(mean))
topic.summary=as.data.frame(topic.summary)
rownames(topic.summary)<-topic.summary$predicted_category
heatmap.2(as.matrix(topic.summary[,-1]), Rowv = FALSE,
scale = "column", key=F,
col = bluered(100),
cexRow = 0.9, cexCol = 0.9, margins = c(8, 8),
trace = "none", density.info = "none")
View(ldaOut.terms_c)
View(ldaOut.terms_c)
View(topics.terms_c)
View(topics.terms_c)
# top 20 terms in each topic
ldaOut.terms_c <- as.matrix(terms(ldaOut_c,20))
# probabilities associated with each topic assignment
topicProbabilities_c <- as.data.frame(ldaOut_c@gamma)
terms.beta_c=ldaOut_c@beta
terms.beta_c=scale(terms.beta_c)
topics.terms_c=NULL
for(i in 1:k_c){
topics.terms_c=rbind(topics.terms_c, ldaOut_c@terms[order(terms.beta_c[i,], decreasing = TRUE)[1:10]])
}
topics.hash=c("Leisure", "achievement", "nature", "affection_2", "enjoy_the_moment", "affection_1", "bonding")
hm.list$ldatopic=as.vector(ldaOut.topics_c)
hm.list$ldahash=topics.hash[ldaOut.topics_c]
colnames(topicProbabilities_c)=topics.hash
hm.list.df=cbind(hm.list, topicProbabilities_c)
par(mar=c(1,1,1,1))
topic.summary_c=tbl_df(hm.list.df)%>%
select(predicted_category, Leisure:bonding)%>%
group_by(predicted_category)%>%
summarise_each(funs(mean))
topic.summary_c=as.data.frame(topic.summary_c)
rownames(topic.summary_c)<-topic.summary_c$predicted_category
heatmap.2(as.matrix(topic.summary_c[,-1]), Rowv = FALSE,
scale = "column", key=F,
col = bluered(100),
cexRow = 0.9, cexCol = 0.9, margins = c(8, 8),
trace = "none", density.info = "none")
ldaOut.terms_c <- as.matrix(terms(ldaOut_c,20))
# probabilities associated with each topic assignment
topicProbabilities_c <- as.data.frame(ldaOut_c@gamma)
terms.beta_c=ldaOut_c@beta
terms.beta_c=scale(terms.beta_c)
topics.terms_c=NULL
for(i in 1:k_c){
topics.terms_c=rbind(topics.terms_c, ldaOut_c@terms[order(terms.beta_c[i,], decreasing = TRUE)[1:21]])
}
topics.hash=c("Leisure", "nature", "nature", "leisure", "enjoy_the_moment", "affection_1", "bonding")
hm.list$ldatopic=as.vector(ldaOut.topics_c)
hm.list$ldahash=topics.hash[ldaOut.topics_c]
colnames(topicProbabilities_c)=topics.hash
hm.list.df=cbind(hm.list, topicProbabilities_c)
par(mar=c(1,1,1,1))
topic.summary_c=tbl_df(hm.list.df)%>%
select(predicted_category, Leisure:bonding)%>%
group_by(predicted_category)%>%
summarise_each(funs(mean))
topics.hash=c("Leisure", "nature", "nature2", "leisure", "enjoy_the_moment", "affection_1", "bonding")
hm.list$ldatopic=as.vector(ldaOut.topics_c)
hm.list$ldahash=topics.hash[ldaOut.topics_c]
colnames(topicProbabilities_c)=topics.hash
hm.list.df=cbind(hm.list, topicProbabilities_c)
par(mar=c(1,1,1,1))
topic.summary_c=tbl_df(hm.list.df)%>%
select(predicted_category, Leisure:bonding)%>%
group_by(predicted_category)%>%
summarise_each(funs(mean))
topic.summary_c=as.data.frame(topic.summary_c)
rownames(topic.summary_c)<-topic.summary_c$predicted_category
heatmap.2(as.matrix(topic.summary_c[,-1]), Rowv = FALSE,
scale = "column", key=F,
col = bluered(100),
cexRow = 0.9, cexCol = 0.9, margins = c(8, 8),
trace = "none", density.info = "none")
ldaOut.terms_c
ldaOut.topics_c <- as.matrix(topics(ldaOut_c))
# top 20 terms in each topic
ldaOut.terms_c <- as.matrix(terms(ldaOut_c,10))
# probabilities associated with each topic assignment
topicProbabilities_c <- as.data.frame(ldaOut_c@gamma)
terms.beta_c=ldaOut_c@beta
terms.beta_c=scale(terms.beta_c)
topics.terms_c=NULL
for(i in 1:k_c){
topics.terms_c=rbind(topics.terms_c, ldaOut_c@terms[order(terms.beta_c[i,], decreasing = TRUE)[1:10]])
}
topics.hash=c("Leisure", "nature", "nature2", "leisure", "enjoy_the_moment", "affection_1", "bonding")
hm.list$ldatopic=as.vector(ldaOut.topics_c)
hm.list$ldahash=topics.hash[ldaOut.topics_c]
colnames(topicProbabilities_c)=topics.hash
hm.list.df=cbind(hm.list, topicProbabilities_c)
par(mar=c(1,1,1,1))
topic.summary_c=tbl_df(hm.list.df)%>%
select(predicted_category, Leisure:bonding)%>%
group_by(predicted_category)%>%
summarise_each(funs(mean))
topic.summary_c=as.data.frame(topic.summary_c)
rownames(topic.summary_c)<-topic.summary_c$predicted_category
heatmap.2(as.matrix(topic.summary_c[,-1]), Rowv = FALSE,
scale = "column", key=F,
col = bluered(100),
cexRow = 0.9, cexCol = 0.9, margins = c(8, 8),
trace = "none", density.info = "none")
